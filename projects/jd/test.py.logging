2020-07-30 11:02:23,097 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Connection timed out after 2000 milliseconds')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Connection timed out after 2000 milliseconds')
2020-07-30 11:02:48,689 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Connection timed out after 2001 milliseconds')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Connection timed out after 2001 milliseconds')
2020-07-30 11:02:52,895 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:02:55,909 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:03,284 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:06,320 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2000 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:12,060 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:15,075 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:23,140 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
2020-07-30 11:03:26,153 - spider.py - MainProcess - [line:215] - ERROR: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 208, in download
    c.perform()
pycurl.error: (28, 'Operation timed out after 2001 milliseconds with 0 out of 0 bytes received')
2020-07-30 19:40:01,872 - spider.py - MainProcess - [line:216] - ERROR: (28, 'Connection timed out after 2000 milliseconds')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 209, in download
    c.perform()
pycurl.error: (28, 'Connection timed out after 2000 milliseconds')
2020-07-30 19:44:33,308 - spider.py - MainProcess - [line:216] - ERROR: (28, 'Connection timed out after 2000 milliseconds')
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 209, in download
    c.perform()
pycurl.error: (28, 'Connection timed out after 2000 milliseconds')
2020-07-31 20:20:09,916 - spider.py - MainProcess - [line:241] - ERROR: request() got an unexpected keyword argument 'proxy'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
TypeError: request() got an unexpected keyword argument 'proxy'
2020-07-31 20:20:33,036 - spider.py - MainProcess - [line:241] - ERROR: request() got an unexpected keyword argument 'proxy'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
TypeError: request() got an unexpected keyword argument 'proxy'
2020-07-31 20:21:15,262 - spider.py - MainProcess - [line:241] - ERROR: request() missing 1 required positional argument: 'method'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
TypeError: request() missing 1 required positional argument: 'method'
2020-07-31 20:21:40,727 - spider.py - MainProcess - [line:241] - ERROR: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 521, in request
    prep.url, proxies, stream, verify, cert
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 698, in merge_environment_settings
    no_proxy = proxies.get('no_proxy') if proxies is not None else None
AttributeError: 'str' object has no attribute 'get'
2020-07-31 20:23:16,588 - spider.py - MainProcess - [line:241] - ERROR: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 521, in request
    prep.url, proxies, stream, verify, cert
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 698, in merge_environment_settings
    no_proxy = proxies.get('no_proxy') if proxies is not None else None
AttributeError: 'str' object has no attribute 'get'
2020-07-31 20:23:26,797 - spider.py - MainProcess - [line:241] - ERROR: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 521, in request
    prep.url, proxies, stream, verify, cert
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 698, in merge_environment_settings
    no_proxy = proxies.get('no_proxy') if proxies is not None else None
AttributeError: 'str' object has no attribute 'get'
2020-07-31 20:23:37,245 - spider.py - MainProcess - [line:241] - ERROR: 'set' object has no attribute 'get'
Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 236, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 521, in request
    prep.url, proxies, stream, verify, cert
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 698, in merge_environment_settings
    no_proxy = proxies.get('no_proxy') if proxies is not None else None
AttributeError: 'set' object has no attribute 'get'
2020-08-03 16:18:23,859 - spider.py - MainProcess - [line:258] - ERROR: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002A81659F080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\util\connection.py", line 61, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "D:\Programs\Python\Python37\lib\socket.py", line 748, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 677, in urlopen
    chunked=chunked,
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 976, in _validate_conn
    conn.connect()
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 308, in connect
    conn = self._new_conn()
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 172, in _new_conn
    self, "Failed to establish a new connection: %s" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002A81659F080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\adapters.py", line 449, in send
    timeout=timeout
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 725, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\util\retry.py", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002A81659F080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 254, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 643, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002A81659F080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
2020-08-03 16:18:28,135 - spider.py - MainProcess - [line:258] - ERROR: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000179DD2AF048>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\util\connection.py", line 61, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "D:\Programs\Python\Python37\lib\socket.py", line 748, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 677, in urlopen
    chunked=chunked,
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 976, in _validate_conn
    conn.connect()
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 308, in connect
    conn = self._new_conn()
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connection.py", line 172, in _new_conn
    self, "Failed to establish a new connection: %s" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000179DD2AF048>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\adapters.py", line 449, in send
    timeout=timeout
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 765, in urlopen
    **response_kw
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\connectionpool.py", line 725, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\urllib3\util\retry.py", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000179DD2AF048>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Users\admin\PycharmProjects\crawl\multiprocess\core\spider.py", line 254, in do_request
    r = s.request(**request)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\sessions.py", line 643, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\admin\PycharmProjects\crawl\env\p3tf2keras\lib\site-packages\requests\adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='item.jd.com', port=443): Max retries exceeded with url: /24809542323.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000179DD2AF048>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
